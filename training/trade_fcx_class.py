# -*- coding: utf-8 -*-
"""trade_FCX_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bc5CPDosj_yobZG7opXYH_YDtBRi6KlI

Installazione librerie necessarie
"""

!pip install stockstats
!pip install --upgrade scikit-learn

import pandas as pd
import matplotlib.pyplot as plt
import stockstats
import xgboost as xgb
import numpy as np
import seaborn as sns

from google.colab import drive
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

"""Data upload from the Drive"""

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Define the file path
file_path = "/content/drive/MyDrive/Colab/data/FCX_3y.csv"

# Load the CSV file into a pandas DataFrame
try:
  df = pd.read_csv(file_path)
  print("File loaded successfully into DataFrame 'df'.")
except FileNotFoundError:
    print(f"Error: The file was not found at the specified path. Please make sure the file exists at '{file_path}'.")
except Exception as e:
    print(f"An error occurred: {e}")

"""Stampa dei valori di chiusura, per contrtollare la correttezza dei dati"""

price = df['close']
fig = plt.figure(figsize=(10, 6))
plt.plot(price)
plt.show()

# Controlliamo il dataset
print(df.head())

"""Aggiunta della variabile "price direction" in modo da rendere piÃ¹ facile il trading. Indica se il prezzo x Ã¨ piÃ¹ grande di x-1 (1) o piu piccolo (0)"""

df['price_direction'] = 0
for i in range(1, len(df)):
    # If the current row's "Price (Close)" is greater than the previous row's "Price (Close)", set "price_direction" to 1
    if df.loc[df.index[i], "close"] > df.loc[df.index[i-1], "close"]:  # Use df.index[i] and df.index[i-1] to access rows by their index labels
        df.loc[df.index[i], "price_direction"] = 1

#print(df.head())

"""Aggiunta di ulteriori indici e variabili per aiutare il modello"""

stock_df = stockstats.StockDataFrame.retype(df, "index")

boll_values = stock_df.get('boll')
rsi = stock_df.get("rsi")
macd = stock_df.get('macd')
chop = stock_df.get('chop')
ppo = stock_df.get('ppo')
vr = stock_df.get('vr')

# Attacco i dati calcolati al DataFrame "data"
df['rsi'] = rsi
df['boll'] = stock_df["boll"]
df['boll_ub'] = stock_df["boll_ub"]
df['boll_lb'] = stock_df["boll_lb"]
df['macd'] = stock_df["macd"]
df['macds'] =stock_df["macds"]
df['macdh'] = stock_df["macdh"]
df['chop'] = stock_df['chop']
df['ppo'] = stock_df['ppo']
df['ppos'] = stock_df['ppos']
df['ppoh'] = stock_df['ppoh']
df["vr"] = stock_df['vr']


# Aggiunta variabili consigliate da chatgpt

df['cci'] = stock_df['cci_14']   # CCI con periodo 14
#df['roc'] = stock_df['roc_10']   # Rate of Change con periodo 10

#data['ema_10'] = stock_df['ema_10']   # EMA 10
#data['ema_50'] = stock_df['ema_50']   # EMA 50
#data['ema_200'] = stock_df['ema_200'] # EMA 200
#data['adx'] = stock_df['adx_14']      # ADX con periodo 14

df['atr'] = stock_df['atr_14']      # ATR con periodo 14

#data['obv'] = stock_df['obv']     # On Balance Volume
df['mfi'] = stock_df['mfi_14']  # Money Flow Index con periodo 14



print(df.head())

"""Inizio preparazione del modello XGBoost

Pulizia dei dati rimuovendo le variabili non necessarie e suddivisione dei dati in "train" e "test"
"""

# Miglioro il rapporto tra classi
#param['scale_pos_weight'] = sum(y_train == 0) / sum(y_train == 1)

# Rimuovere colonne inutili se presenti
columns_to_drop = ["timestamp","symbol", "close", "open", "high", "low"]
columns_to_drop = [col for col in columns_to_drop if col in df.columns]  # Check if column exists
df = df.drop(columns=columns_to_drop)

# Replace infinite values with NaN
df = df.replace([np.inf, -np.inf], np.nan)

rows_before = df.shape[0]  # Numero di righe prima
df = df.dropna().reset_index(drop=True)
rows_after = df.shape[0]   # Numero di righe dopo

print(f"Righe eliminate: {rows_before - rows_after}")  # Controllo che solo la prima riga sia stata eliminata

# Definiamo X (feature) e y (target)
X = df.drop(columns=["price_direction"])  # Feature
y = df["price_direction"]  # Target

# Controlliamo la forma dei dati
print("Shape di X:", X.shape)
print("Shape di y:", y.shape)

# Visualizziamo le prime righe di X
print(X.head())

# Dividiamo il dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Controlliamo la dimensione dei dataset
print(f"Train set: {X_train.shape}, Test set: {X_test.shape}")

# Convert 'timestamp' column to numerical representation (e.g., Unix timestamp)
#X_train['timestamp'] = pd.to_numeric(X_train['timestamp'])
#X_test['timestamp'] = pd.to_numeric(X_test['timestamp'])

# Cambio del formato affinchÃ¨ lavori meglio con xgboost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

"""Inizio del TRAINING"""

# Definizione dei parametri del modello
param = {
    'max_depth': 6,         # Aumentiamo la profonditÃ  per catturare pattern piÃ¹ complessi
    'eta': 0.05,            # Riduciamo il learning rate per migliorare la generalizzazione
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'colsample_bytree': 0.8, # Seleziona lâ€™80% delle feature ad ogni split per ridurre overfitting
    'subsample': 0.8,       # Usa solo l'80% dei dati per ogni albero (migliora robustezza)
    'n_estimators': 200,    # Aumentiamo il numero di alberi
    'gamma': 0.01,            # PenalitÃ  per il gain nei nodi
    'reg_lambda': 0.05       # Regularization L2
}

num_round = 1000  # Numero massimo di iterazioni (early stopping lo fermerÃ  prima)
eval_list = [(dtrain, 'train'), (dtest, 'eval')]

# Training con early stopping
bst = xgb.train(
    params=param,
    dtrain=dtrain,
    num_boost_round=num_round,
    evals=eval_list,
    early_stopping_rounds=10  # Stop se non migliora per 10 iterazioni consecutive
)

# Salvataggio del miglior modello trovato
bst.save_model('classification_xgboost_model.json')

print(f" Modello addestrato e salvato! Migliore iterazione: {bst.best_iteration}")

"""Fase di PREDICTION"""

# Ricarichiamo il miglior modello
bst_loaded = xgb.Booster()
bst_loaded.load_model('classification_xgboost_model.json')

# Salvo sul drive il modello
bst_loaded.save_model('/content/drive/MyDrive/Colab/classification_xgboost_model.json')

# Facciamo le predizioni sui dati di test
y_pred_prob = bst_loaded.predict(dtest, iteration_range=(0, bst_loaded.best_iteration + 1))

# Convertiamo le probabilitÃ  in classi (0 = short, 1 = long)
y_pred = (y_pred_prob >= 0.5).astype(int)

# Stampiamo le prime 10 predizioni
print("Prime 10 predizioni:")
print(y_pred[:10])

"""Valutazione del modello appena allenato"""

# Calcoliamo le metriche di valutazione
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Stampiamo i risultati
print(f"ðŸŽ¯ Accuratezza: {accuracy:.4f}")
print(f"ðŸŽ¯ Precisione: {precision:.4f}")
print(f"ðŸŽ¯ Recall: {recall:.4f}")
print(f"ðŸŽ¯ F1-score: {f1:.4f}")

# Creiamo la matrice di confusione per capire meglio che tipo di errori fa
cm = confusion_matrix(y_test, y_pred)

# Visualizziamola con Seaborn
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Short (0)", "Long (1)"], yticklabels=["Short (0)", "Long (1)"])
plt.xlabel("Predetto")
plt.ylabel("Reale")
plt.title("Matrice di Confusione")
plt.show()